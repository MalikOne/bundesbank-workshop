---
title: "Basics and CART"
output: html_notebook
---

This notebook contains code from 1-basics-cart.R and the corresponding output.

## Setup

```{r}
library(ggplot2)
library(ggmap)
library(GGally)
library(rpart)
library(partykit)
library(pdp)
library(caret)
```

```{r}
load("FrankfurtMain.Rda")

set.seed(7345)
train <- sample(1:nrow(fr_immo), 0.8*nrow(fr_immo))
fr_test <- fr_immo[-train,]
fr_train <- fr_immo[train,]
```

## Some data exploration

After all packages and data have been loaded and set up, we might want to explore the training set using graphs. Here is a nice plot using `ggpairs` from the `GGally` package.

```{r, results="hide", fig.align="center"}
ggpairs(fr_train[,c(2:4,20)], lower = list(continuous = "smooth"))
```

## Grow and prune tree

Our task is to predict rent using $m^2$, rooms, location and distance to city center as features. In order to grow a regression tree, `rpart` is used here, which follows the CART idea. 

```{r}
set.seed(6342)
f_tree <- rpart(rent ~ m2 + rooms + lon + lat + dist_to_center, data = fr_train, cp = 0.0025)
printcp(f_tree)
```

Given the grown tree, `printcp` and `plotcp` help us to determine the best subtree, whereas `Root node error` times `xerror` gives us the estimated test error for each subtree based on cross-validation. 

```{r, fig.align="center"}
plotcp(f_tree)
```

We can get the best subtree with the `prune` function, here using the one standard error rule.

```{r}
minx <- which.min(f_tree$cptable[,"xerror"])
minxse <- f_tree$cptable[minx,"xerror"] + f_tree$cptable[minx,"xstd"]
minse <- which.min(abs(f_tree$cptable[1:minx,"xerror"] - minxse))
mincp <- f_tree$cptable[minse,"CP"]
p_tree <- prune(f_tree, cp = mincp)
```

## Plots

An advantage of trees is that they can be easily interpreted by a simple plot. The `party` package produces nice tree plots.

```{r, fig.align="center"}
prty_tree <- as.party(p_tree)
plot(prty_tree, gp = gpar(fontsize = 6))
```

If we are interested in the prediction surface of the tree, partial dependence plots using e.g. the `pdp` package can be useful.

```{r, fig.align="center"}
pdp1 <- partial(p_tree, pred.var = "m2")
plotPartial(pdp1, rug = T, train = fr_train, alpha = 0.3)
```

```{r, fig.align="center"}
pdp2 <- partial(p_tree, pred.var = c("m2", "dist_to_center"))
plotPartial(pdp2, levelplot = F, drape = T, colorkey = F, screen = list(z = 40, x = -60))
```

## Prediction

Finally, we can use the pruned tree in order to predict the outcome in the holdout (test) set. Then, `postResample` produces basic performance measures.

```{r}
y_tree <- predict(p_tree, newdata = fr_test)
postResample(pred = y_tree, obs = fr_test$rent)
```